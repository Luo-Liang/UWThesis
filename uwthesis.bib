@inproceedings{alexnet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@misc{RESNET,
Author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
Title = {Deep Residual Learning for Image Recognition},
Year = {2015},
Eprint = {arXiv:1512.03385},
}

@article{VGGNet,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{ZFNet,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@article{GoogleNet,
   title={Going deeper with convolutions},
   ISBN={9781467369640},
   url={http://dx.doi.org/10.1109/CVPR.2015.7298594},
   DOI={10.1109/cvpr.2015.7298594},
   journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Szegedy, Christian and Wei Liu and Yangqing Jia and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
   year={2015},
   month={Jun}
}

@misc{bert,
    title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    year={2018},
    eprint={1810.04805},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{GPT2,
author = {OpenAI},
title = {Better Language Models and Their Implications},
howpublished = {https://openai.com/blog/better-language-models},
month = {},
year = {},
note = {(Accessed on 03/28/2019)}
}

@article{gpt2Time,
author = {Reddit},
title = {OpenAI: Better Language Models and Their Implications: Machine Learning},
howpublished = {https://www.reddit.com/r/MachineLearning/comments/aqlzde/r_openai_better_language_models_and_their/},
month = {},
year = {},
note = {(Accessed on 03/28/2019)}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@misc{mirhoseini2017device,
    title={Device Placement Optimization with Reinforcement Learning},
    author={Azalia Mirhoseini and Hieu Pham and Quoc V. Le and Benoit Steiner and Rasmus Larsen and Yuefeng Zhou and Naveen Kumar and Mohammad Norouzi and Samy Bengio and Jeff Dean},
    year={2017},
    eprint={1706.04972},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{AIandCom3:online,
author = {},
title = {AI and Compute},
howpublished = {https://openai.com/blog/ai-and-compute},
month = {},
year = {},
note = {(Accessed on 03/28/2019)}
}

@misc{zoph2016neural,
    title={Neural Architecture Search with Reinforcement Learning},
    author={Barret Zoph and Quoc V. Le},
    year={2016},
    eprint={1611.01578},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Chollet_2017,
   title={Xception: Deep Learning with Depthwise Separable Convolutions},
   ISBN={9781538604571},
   url={http://dx.doi.org/10.1109/CVPR.2017.195},
   DOI={10.1109/cvpr.2017.195},
   journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Chollet, Francois},
   year={2017},
   month={Jul}
}

@misc{IDCRepor64:online,
author = {},
title = {IDC Report: The New Need for Speed in the Datacenter Network},
howpublished = {https://www.cisco.com/c/dam/en/us/products/collateral/switches/nexus-9000-series-switches/white-paper-c11-734328.pdf},
month = {},
year = {},
note = {(Accessed on 04/03/2019)}
}

@misc{EC2Netwo58:online,
author = {},
title = {EC2 Network Performance Cheat Sheet | cloudonaut},
howpublished = {https://cloudonaut.io/ec2-network-performance-cheat-sheet/},
month = {},
year = {},
note = {(Accessed on 04/03/2019)}
}

@misc{Introduc72:online,
author = {},
title = {Introducing the new HB and HC Azure VM sizes for HPC | Blog | Microsoft Azure},
howpublished = {https://azure.microsoft.com/en-us/blog/introducing-the-new-hb-and-hc-azure-vm-sizes-for-hpc/},
month = {},
year = {},
note = {(Accessed on 04/03/2019)}
}

@misc{NewC5nIn20:online,
author = {},
title = {New C5n Instances with 100 Gbps Networking | AWS News Blog},
howpublished = {https://aws.amazon.com/blogs/aws/new-c5n-instances-with-100-gbps-networking/},
month = {},
year = {},
note = {(Accessed on 04/03/2019)}
}

@article{jayarajan2019priority,
	title={Priority-based parameter propagation for distributed DNN training},
	author={Jayarajan, Anand and Wei, Jinliang and Gibson, Garth and Fedorova, Alexandra and Pekhimenko, Gennady},
	journal={arXiv preprint arXiv:1905.03960},
	year={2019}
}

@misc{sapio2019scaling,
    title={Scaling Distributed Machine Learning with In-Network Aggregation},
    author={Amedeo Sapio and Marco Canini and Chen-Yu Ho and Jacob Nelson and Panos Kalnis and Changhoon Kim and Arvind Krishnamurthy and Masoud Moshref and Dan R. K. Ports and Peter Richtárik},
    year={2019},
    eprint={1903.06701},
    archivePrefix={arXiv},
    primaryClass={cs.DC}
}

@misc{moreau2018vta,
    title={VTA: An Open Hardware-Software Stack for Deep Learning},
    author={Thierry Moreau and Tianqi Chen and Ziheng Jiang and Luis Ceze and Carlos Guestrin and Arvind Krishnamurthy},
    year={2018},
    eprint={1807.04188},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Roesch_2018,
   title={Relay: a new IR for machine learning frameworks},
   ISBN={9781450358347},
   url={http://dx.doi.org/10.1145/3211346.3211348},
   DOI={10.1145/3211346.3211348},
   journal={Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages  - MAPL 2018},
   publisher={ACM Press},
   author={Roesch, Jared and Lyubomirsky, Steven and Weber, Logan and Pollock, Josh and Kirisame, Marisa and Chen, Tianqi and Tatlock, Zachary},
   year={2018}
}

@inproceedings{phubsocc,
  title={Parameter hub: a rack-scale parameter server for distributed deep neural network training},
  author={Luo, Liang and Nelson, Jacob and Ceze, Luis and Phanishayee, Amar and Krishnamurthy, Arvind},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={41--54},
  year={2018},
  organization={ACM}
}

@article{phubsysml,
  title={Parameter Hub: High Performance Parameter Servers for Efficient Distributed Deep Neural Network Training},
  author={Liang Luo and Jacob S Nelson and Luis Ceze and Amar Phanishayee and Arvind Krishnamurthy},
  journal={SysML},
  year={2018},
  volume={abs/1801.09805}
}

@inproceeding{backprop,
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	chapter = {Learning Representations by Back-propagating Errors},
	title = {Neurocomputing: Foundations of Research},
	editor = {Anderson, James A. and Rosenfeld, Edward},
	year = {1988},
	isbn = {0-262-01097-6},
	pages = {696--699},
	numpages = {4},
	url = {http://dl.acm.org/citation.cfm?id=65669.104451},
	acmid = {104451},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}

@misc{MXNetont0:online,
	author = {},
	title = {MXNet on the Cloud — mxnet documentation},
	howpublished = {https://mxnet.incubator.apache.org/faq/cloud.html?highlight=ec2},
	month = {},
	year = {},
	note = {(Accessed on 05/09/2018)}
}

@article{hu2003evaluation,
	title={Evaluation and characterization of available bandwidth probing techniques},
	author={Hu, Ningning and Steenkiste, Peter},
	journal={IEEE journal on Selected Areas in Communications},
	volume={21},
	number={6},
	pages={879--894},
	year={2003},
	publisher={IEEE}
}

@techreport{hu2002estimating,
	title={Estimating available bandwidth using packet pair probing},
	author={Hu, Ningning and Steenkiste, Peter},
	year={2002},
	institution={CARNEGIE-MELLON UNIV PITTSBURGH PA SCHOOL OF COMPUTER SCIENCE}
}

@misc{Distribu25:online,
	author = {},
	title = {Distributed Training | Caffe2},
	howpublished = {https://caffe2.ai/docs/distributed-training.html},
	month = {},
	year = {},
	note = {(Accessed on 05/09/2018)}
}

@misc{ApacheMX19:online,
	author = {},
	title = {Apache MXNet on AWS},
	howpublished = {https://aws.amazon.com/mxnet/},
	month = {},
	year = {},
	note = {(Accessed on 05/09/2018)}
}


@MISC{BerkleyCS294,
	title = {Scaling Deep Learning},
	howpublished = {https://berkeley-deep-learning.github.io/cs294-131-s17/slides/Catanzaro_Berkeley_CS294.pdf}
}

@article{DBLP:journals/corr/abs-1712-01887,
	author    = {Yujun Lin and
	Song Han and
	Huizi Mao and
	Yu Wang and
	William J. Dally},
	title     = {Deep Gradient Compression: Reducing the Communication Bandwidth for
	Distributed Training},
	journal   = {CoRR},
	volume    = {abs/1712.01887},
	year      = {2017},
	url       = {http://arxiv.org/abs/1712.01887},
	archivePrefix = {arXiv},
	eprint    = {1712.01887},
	timestamp = {Wed, 03 Jan 2018 12:33:17 +0100},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-01887},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/SzegedyIV16,
	author    = {Christian Szegedy and
	Sergey Ioffe and
	Vincent Vanhoucke},
	title     = {Inception-v4, Inception-ResNet and the Impact of Residual Connections
	on Learning},
	journal   = {CoRR},
	volume    = {abs/1602.07261},
	year      = {2016},
	url       = {http://arxiv.org/abs/1602.07261},
	archivePrefix = {arXiv},
	eprint    = {1602.07261},
	timestamp = {Wed, 07 Jun 2017 14:42:58 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/SzegedyIV16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/XieKX14,
	author    = {Pengtao Xie and
	Jin Kyu Kim and
	Eric P. Xing},
	title     = {Large Scale Distributed Multiclass Logistic Regression},
	journal   = {CoRR},
	volume    = {abs/1409.5705},
	year      = {2014},
	url       = {http://arxiv.org/abs/1409.5705},
	archivePrefix = {arXiv},
	eprint    = {1409.5705},
	timestamp = {Wed, 07 Jun 2017 14:41:33 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/XieKX14},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{TFPS,
	title = {Distributed {TensorFlow}},
	howpublished = {https://www.tensorflow.org/deploy/distributed},
}

@misc{Gloo,
	title = {Facebook {Gloo} collectives algorithms library},
	howpublished = {https://github.com/facebookincubator/gloo/blob/master/docs/algorithms.md},
}

@misc{PSLite,
	title = {A light and efficient implementation of the parameter server framework. It provides clean yet powerful APIs. For example, a worker node can communicate with the server nodes by},
	howpublished = {https://github.com/dmlc/ps-lite},
}

@misc{AMDEpyc,
	title = {{AMD EPYC}},
	howpublished = {http://www.amd.com/en/products/epyc}
}

@misc{IBMAnnouncement,
	title = {{IBM} {Research} achieves record deep learning performance with new software technology },
	howpublished = {https://www.ibm.com/blogs/research/2017/08/distributed-deep-learning/}
}

@inproceedings{GeePS,
	author = {Cui, Henggang and Zhang, Hao and Ganger, Gregory R. and Gibbons, Phillip B. and Xing, Eric P.},
	title = {{GeePS}: Scalable Deep Learning on Distributed GPUs with a GPU-specialized Parameter Server},
	booktitle = {Proceedings of the Eleventh European Conference on Computer Systems},
	series = {EuroSys '16},
	year = {2016},
	isbn = {978-1-4503-4240-7},
	location = {London, United Kingdom},
	pages = {4:1--4:16},
	articleno = {4},
	numpages = {16},
	url = {http://doi.acm.org/10.1145/2901318.2901323},
	doi = {10.1145/2901318.2901323},
	acmid = {2901323},
	publisher = {ACM},
	address = {New York, NY, USA},
} 

@article{DBLP:journals/corr/DaiKWHGX14,
	author    = {Wei Dai and
	Abhimanu Kumar and
	Jinliang Wei and
	Qirong Ho and
	Garth A. Gibson and
	Eric P. Xing},
	title     = {High-Performance Distributed {ML} at Scale through Parameter Server
	Consistency Models},
	journal   = {CoRR},
	volume    = {abs/1410.8043},
	year      = {2014},
	url       = {http://arxiv.org/abs/1410.8043},
	archivePrefix = {arXiv},
	eprint    = {1410.8043},
	timestamp = {Wed, 07 Jun 2017 14:41:51 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/DaiKWHGX14},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{zmq,
	title = {{ZMQ} Distributed Messaging},
	howpublished = {http://zeromq.org/}
}


@misc{subminibatching,
title= {How to use sub mini batching?},
howpublished={https://github.com/Microsoft/CNTK/issues/1938}
}
	
@misc{EpycBenchmark,
title = {{Epyc} Benchmarks},
howpublished = {https://www.amd.com/en/products/epyc-benchmarks}
}

@misc{Chainer,
	title = {Performance of Distributed Deep Learning using {ChainerMN}},
	howpublished = {https://chainer.org/general/2017/02/08/Performance-of-Distributed-Deep-Learning-Using-ChainerMN.html}
}

@misc{ResNextRepo,
	title = {{ResNeXt}: {Aggregated} Residual Transformations for Deep Neural Networks},
	howpublished = {https://github.com/facebookresearch/ResNeXt}
}

@misc{ec2BW,
	title = {EC2Instances.info {Easy} {Amazon} {EC2} Instance Comparison},
	howpublished = {https://www.ec2instances.info/?region=us-west-2}
}




@article{DBLP:journals/corr/SzegedyVISW15,
	author    = {Christian Szegedy and
	Vincent Vanhoucke and
	Sergey Ioffe and
	Jonathon Shlens and
	Zbigniew Wojna},
	title     = {Rethinking the {Inception} Architecture for Computer Vision},
	journal   = {CoRR},
	volume    = {abs/1512.00567},
	year      = {2015},
	url       = {http://arxiv.org/abs/1512.00567},
	archivePrefix = {arXiv},
	eprint    = {1512.00567},
	timestamp = {Wed, 07 Jun 2017 14:40:22 +0200},
	biburl    = {http://dblp.org/rec/bib/journals/corr/SzegedyVISW15},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@misc{AzureWin5:online,
	author = {},
	title = {{Azure} {Windows} {VM} sizes - {HPC}},
	howpublished = {https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-hpc},
	month = {},
	year = {},
	note = {(Accessed on 01/11/2018)}
}

@article{chen2016training,
title={Training deep nets with sublinear memory cost},
author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
journal={arXiv preprint arXiv:1604.06174},
year={2016}
}

@article{alannetwork,
	title={Network Evolution for DNNs},
	author={Alan, Michael and Panda, Aurojit and Bottini, Domenic and Jian, Lisa and Kumar, Pranay and Shenker, Scott}
}

@article{DBLP:journals/corr/abs-1712-03890,
	author    = {Christopher Streiffer and
	Huan Chen and
	Theophilus Benson and
	Asim Kadav},
	title     = {DeepConfig: Automating Data Center Network Topologies Management with
	Machine Learning},
	journal   = {CoRR},
	volume    = {abs/1712.03890},
	year      = {2017},
	url       = {http://arxiv.org/abs/1712.03890},
	archivePrefix = {arXiv},
	eprint    = {1712.03890},
	timestamp = {Wed, 03 Jan 2018 12:33:17 +0100},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-03890},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{youspeeding,
	title={Speeding up ImageNet Training on Supercomputers},
	author={You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt}
}

@article{koliousiscrossbow,
	title={CrossBow: Scaling Deep Learning on Multi-GPU Servers},
	author={Koliousis, Alexandros and Watcharapichat, Pijika and Weidlich, Matthias and Costa, Paolo and Pietzuch, Peter}
}

@ARTICLE{zhudnn,
	author = {{Zhu}, H. and {Akrout}, M. and {Zheng}, B. and {Pelegris}, A. and 
	{Phanishayee}, A. and {Schroeder}, B. and {Pekhimenko}, G.},
	title = "{TBD: Benchmarking and Analyzing Deep Neural Network Training}",
	journal = {ArXiv e-prints},
	archivePrefix = "arXiv",
	eprint = {1803.06905},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	year = 2018,
	month = mar,
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180306905Z},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{wang2018optimal,
	title={Optimal Message Scheduling for Aggregation},
	author={Wang, Leyuan and Li, Mu and Liberty, Edo and Smola, Alex J},
	journal={NETWORKS},
	volume={2},
	number={3},
	pages={2--3},
	year={2018}
}

@misc{baidures3:online,
	author = {},
	title = {baidu-research/baidu-allreduce},
	howpublished = {https://github.com/baidu-research/baidu-allreduce},
	month = {},
	year = {},
	note = {(Accessed on 05/14/2018)}
}

@article{DBLP:journals/corr/abs-1802-05799,
	author    = {Alexander Sergeev and
	Mike Del Balso},
	title     = {Horovod: fast and easy distributed deep learning in TensorFlow},
	journal   = {CoRR},
	volume    = {abs/1802.05799},
	year      = {2018},
	url       = {http://arxiv.org/abs/1802.05799},
	archivePrefix = {arXiv},
	eprint    = {1802.05799},
	timestamp = {Thu, 01 Mar 2018 19:20:48 +0100},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-05799},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{1608.07249,
	Author = {Shaohuai Shi and Qiang Wang and Pengfei Xu and Xiaowen Chu},
	Title = {Benchmarking State-of-the-Art Deep Learning Software Tools},
	Year = {2016},
	Eprint = {arXiv:1608.07249},
}

@article{chen2015mxnet,
	title={{MXNet}: A flexible and efficient machine learning library for heterogeneous distributed systems},
	author={Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
	journal={arXiv preprint arXiv:1512.01274},
	year={2015}
}

@article{luomotivating,
	title={Motivating In-Network Aggregation for Distributed Deep Neural Network Training},
	author={Luo, Liang and Liu, Ming and Nelson, Jacob and Ceze, Luis and Phanishayee, Amar and Krishnamurthy, Arvind},
	year = {2017},
	booktitle = {WAX 2017}
}

@inproceedings{43837,
	title	= {Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network},
	author	= {Arjun Singh and Joon Ong and Amit Agarwal and Glen Anderson and Ashby Armistead and Roy Bannon and Seb Boving and Gaurav Desai and Bob Felderman and Paulie Germano and Anand Kanagala and Jeff Provost and Jason Simmons and Eiichi Tanda and Jim Wanderer and Urs Hölzle and Stephen Stuart and Amin Vahdat},
	year	= {2015},
	booktitle	= {Sigcomm '15}
}

@inproceedings{Mysore2009PortLandAS,
	title={PortLand: a scalable fault-tolerant layer 2 data center network fabric},
	author={Radhika Niranjan Mysore and Andreas Pamboris and Nathan Farrington and Nelson Huang and Pardis Miri and Sivasankar Radhakrishnan and Vikram Subramanya and Amin Vahdat},
	booktitle={SIGCOMM},
	year={2009}
}

@article{Roy2015InsideTS,
	title={Inside the Social Network's (Datacenter) Network},
	author={Arjun Roy and Hongyi Zeng and Jasmeet Bagga and George Porter and Alex C. Snoeren},
	journal={Computer Communication Review},
	year={2015},
	volume={45},
	pages={123-137}
}

@inproceedings{vl2-a-scalable-and-flexible-data-center-network,
	author = {Greenberg, Albert and Hamilton, James R. and Jain, Navendu and Kandula, Srikanth and Kim, Changhoon and Lahiri, Parantap and Maltz, Dave and Patel, Parveen and Sengupta, Sudipta},
	title = {VL2: A Scalable and Flexible Data Center Network},
	booktitle = {},
	year = {2009},
	month = {August},
	abstract = {
	
	To be agile and cost effective, data centers should allow dynamic resource allocation across large ver pools. In particular, the data center network should enable any server to be assigned to any service. To meet these goals, we present VL2, a practical network architecture that scales to support huge data centers with uniform high capacity between servers, performance isolation between services, and Ethernet layer-2 semantics. VL2 uses (1) flat addressing to allow service instances to be placed anywhere in the network, (2) Valiant Load Balancing to spread traffic uniformly across network paths, and (3) end-system based address resolution to scale to large server pools, without introducing complexity to the network control plane. VL2’s design is driven by detailed measurements of traffic and fault data from a large operational cloud service provider. VL2’s implementation leverages proven network technologies, already available at low cost in high-speed hardware implementations, to build a scalable and reliable network architecture. As a result, VL2 networks can be deployed today, and we have built a working prototype. We evaluate the merits of the VL2 design using measurement, analysis, and experiments. Our VL2 prototype shuffles 2.7 TB of data among 75 servers in 395 seconds – sustaining a rate that is 94% of the maximum possible.
	
	Invited paper in Research Highlights section of the newly re-formatted Communications of the ACM (CACM).
	
	
	},
	publisher = {Association for Computing Machinery, Inc.},
	url = {https://www.microsoft.com/en-us/research/publication/vl2-a-scalable-and-flexible-data-center-network/},
	address = {},
	pages = {},
	journal = {},
	volume = {},
	chapter = {},
	isbn = {},
}

@misc{Placemen61:online,
	author = {},
	title = {Placement Groups - Amazon Elastic Compute Cloud},
	howpublished = {https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster},
	month = {},
	year = {},
	note = {(Accessed on 05/11/2018)}
}

@inproceedings{Wei:2015:MCC:2806777.2806778,
	author = {Wei, Jinliang and Dai, Wei and Qiao, Aurick and Ho, Qirong and Cui, Henggang and Ganger, Gregory R. and Gibbons, Phillip B. and Gibson, Garth A. and Xing, Eric P.},
	title = {Managed Communication and Consistency for Fast Data-parallel Iterative Analytics},
	booktitle = {Proceedings of the Sixth ACM Symposium on Cloud Computing},
	series = {SoCC '15},
	year = {2015},
	isbn = {978-1-4503-3651-2},
	location = {Kohala Coast, Hawaii},
	pages = {381--394},
	numpages = {14},
	url = {http://doi.acm.org/10.1145/2806777.2806778},
	doi = {10.1145/2806777.2806778},
	acmid = {2806778},
	publisher = {ACM},
	address = {New York, NY, USA},
} 


@inproceedings{SSP,
	title={More effective distributed {ML} via a stale synchronous parallel parameter server},
	author={Ho, Qirong and Cipar, James and Cui, Henggang and Lee, Seunghak and Kim, Jin Kyu and Gibbons, Phillip B and Gibson, Garth A and Ganger, Greg and Xing, Eric P},
	booktitle={Advances in neural information processing systems},
	pages={1223--1231},
	year={2013}
}

@inproceedings{BSP,
	title={Exploiting Bounded Staleness to Speed Up Big Data Analytics.},
	author={Cui, Henggang and Cipar, James and Ho, Qirong and Kim, Jin Kyu and Lee, Seunghak and Kumar, Abhimanu and Wei, Jinliang and Dai, Wei and Ganger, Gregory R and Gibbons, Phillip B and others},
	booktitle={USENIX Annual Technical Conference},
	pages={37--48},
	year={2014}
}

@inproceedings{dai2015high,
	title={High-Performance Distributed {ML} at Scale through Parameter Server Consistency Models.},
	author={Dai, Wei and Kumar, Abhimanu and Wei, Jinliang and Ho, Qirong and Gibson, Garth A and Xing, Eric P},
	booktitle={AAAI},
	pages={79--87},
	year={2015}
}

@inproceedings{cui2014exploiting,
	title={Exploiting iterative-ness for parallel ML computations},
	author={Cui, Henggang and Tumanov, Alexey and Wei, Jinliang and Xu, Lianghong and Dai, Wei and Haber-Kucharsky, Jesse and Ho, Qirong and Ganger, Gregory R and Gibbons, Phillip B and Gibson, Garth A and others},
	booktitle={Proceedings of the ACM Symposium on Cloud Computing},
	pages={1--14},
	year={2014},
	organization={ACM}
}

@inproceedings{recht2011hogwild,
	title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
	author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
	booktitle={Advances in neural information processing systems},
	pages={693--701},
	year={2011}
}

@inproceedings{cntk1bt,
	author = {Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
	title = {1-Bit Stochastic Gradient Descent and Application to Data-Parallel Distributed Training of Speech {DNNs}},
	booktitle = {Interspeech 2014},
	year = {2014},
	month = {September},
	abstract = {
	
	We show empirically that in SGD training of deep neural networks, one can, at no or nearly no loss of accuracy, quantize the gradients aggressively—to but one bit per value—if the quantization error is carried forward across minibatches (error feedback). This size reduction makes it feasible to parallelize SGD through data-parallelism with fast processors like recent GPUs.
	
	We implement data-parallel deterministically distributed SGD by combining this finding with AdaGrad, automatic minibatch-size selection, double buffering, and model parallelism. Unexpectedly, quantization benefits AdaGrad, giving a small accuracy gain.
	
	For a typical Switchboard DNN with 46M parameters, we reach computation speeds of 27k frames per second (kfps) when using 2880 samples per minibatch, and 51kfps with 16k, on a server with 8 K20X GPUs. This corresponds to speed-ups over a single GPU of 3.6 and 6.3, respectively. 7 training passes over 309h of data complete in under 7h. A 160M-parameter model training processes 3300h of data in under 16h on 20 dual-GPU servers—a 10 times speed-up—albeit at a small accuracy loss.
	
	
	},
	publisher = {},
	url = {https://www.microsoft.com/en-us/research/publication/1-bit-stochastic-gradient-descent-and-application-to-data-parallel-distributed-training-of-speech-dnns/},
	address = {},
	pages = {},
	journal = {},
	volume = {},
	chapter = {},
	isbn = {},
}

@article{ImageNetIn1Hour,
	title={Accurate, Large Minibatch {SGD}: Training {ImageNet} in 1 Hour},
	author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	journal={arXiv preprint arXiv:1706.02677},
	year={2017}
}

@inproceedings {poseidon,
	author = {Hao Zhang and Zeyu Zheng and Shizhen Xu and Wei Dai and Qirong Ho and Xiaodan Liang and Zhiting Hu and Jinliang Wei and Pengtao Xie and Eric P. Xing},
	title = {Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on {GPU} Clusters},
	booktitle = {2017 {USENIX} Annual Technical Conference ({USENIX} {ATC} 17)},
	year = {2017},
	isbn = {978-1-931971-38-6},
	address = {Santa Clara, CA},
	pages = {181--193},
	url = {https://www.usenix.org/conference/atc17/technical-sessions/presentation/zhang},
	publisher = {{USENIX} Association},
}

@inproceedings{firecaffe,
	title={Firecaffe: near-linear acceleration of deep neural network training on compute clusters},
	author={Iandola, Forrest N and Moskewicz, Matthew W and Ashraf, Khalid and Keutzer, Kurt},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={2592--2600},
	year={2016}
}


@inproceedings{ps1,
 author = {Li, Mu and Andersen, David G. and Park, Jun Woo and Smola, Alexander J. and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J. and Su, Bor-Yiing},
 title = {Scaling Distributed Machine Learning with the Parameter Server},
 booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
 series = {OSDI'14},
 year = {2014},
 isbn = {978-1-931971-16-4},
 location = {Broomfield, CO},
 pages = {583--598},
 numpages = {16},
 url = {http://dl.acm.org/citation.cfm?id=2685048.2685095},
 acmid = {2685095},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
} 

@inproceedings{ps2,
 author = {Li, Mu and Andersen, David G. and Smola, Alexander and Yu, Kai},
 title = {Communication Efficient Distributed Machine Learning with the Parameter Server},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems},
 series = {NIPS'14},
 year = {2014},
 location = {Montreal, Canada},
 pages = {19--27},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2968826.2968829},
 acmid = {2968829},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{ps0,
 author = {Smola, Alexander and Narayanamurthy, Shravan},
 title = {An Architecture for Parallel Topic Models},
 journal = {Proc. VLDB Endow.},
 issue_date = {September 2010},
 volume = {3},
 number = {1-2},
 month = sep,
 year = {2010},
 issn = {2150-8097},
 pages = {703--710},
 numpages = {8},
 url = {http://dx.doi.org/10.14778/1920841.1920931},
 doi = {10.14778/1920841.1920931},
 acmid = {1920931},
 publisher = {VLDB Endowment},
} 

@article{ps3,
	author = {Zhang, Ce and R{\'e}, Christopher},
	title = {DimmWitted: A Study of Main-memory Statistical Analytics},
	journal = {Proc. VLDB Endow.},
	issue_date = {August 2014},
	volume = {7},
	number = {12},
	month = aug,
	year = {2014},
	issn = {2150-8097},
	pages = {1283--1294},
	numpages = {12},
	url = {http://dx.doi.org/10.14778/2732977.2733001},
	doi = {10.14778/2732977.2733001},
	acmid = {2733001},
	publisher = {VLDB Endowment},
} 

@misc{CloudTPU88:online,
	author = {},
	title = {Cloud TPUs - ML accelerators for TensorFlow  |  Google Cloud},
	howpublished = {https://cloud.google.com/tpu/},
	month = {},
	year = {},
	note = {(Accessed on 05/16/2018)}
}

@misc{MachineL65:online,
	author = {},
	title = {Machine Learning | Microsoft Azure},
	howpublished = {https://azure.microsoft.com/en-us/services/machine-learning-studio/},
	month = {},
	year = {},
	note = {(Accessed on 05/16/2018)}
}

@inproceedings{sharp,
 author = {Graham, Richard L. and Bureddy, Devendar and Lui, Pak and Rosenstock, Hal and Shainer, Gilad and Bloch, Gil and Goldenerg, Dror and Dubman, Mike and Kotchubievsky, Sasha and Koushnir, Vladimir and Levi, Lion and Margolin, Alex and Ronen, Tamir and Shpiner, Alexander and Wertheim, Oded and Zahavi, Eitan},
 title = {Scalable Hierarchical Aggregation Protocol {(SHArP)}: A Hardware Architecture for Efficient Data Reduction},
 booktitle = {Proceedings of the First Workshop on Optimization of Communication in HPC},
 series = {COM-HPC '16},
 year = {2016},
 isbn = {978-1-5090-3829-9},
 location = {Salt Lake City, Utah},
 pages = {1--10},
 numpages = {10},
 url = {https://doi.org/10.1109/COM-HPC.2016.6},
 doi = {10.1109/COM-HPC.2016.6},
 acmid = {3018059},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
} 

@inproceedings {switchkv,
author = {Xiaozhou Li and Raghav Sethi and Michael Kaminsky and David G. Andersen and Michael J. Freedman},
title = {Be Fast, Cheap and in Control with {SwitchKV}},
booktitle = {13th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 16)},
year = {2016},
isbn = {978-1-931971-29-4},
address = {Santa Clara, CA},
pages = {31--44},
url = {https://www.usenix.org/conference/nsdi16/technical-sessions/presentation/li-xiaozhou},
publisher = {{USENIX} Association},
}

@article{incbricks,
 author = {Liu, Ming and Luo, Liang and Nelson, Jacob and Ceze, Luis and Krishnamurthy, Arvind and Atreya, Kishore},
 title = {{IncBricks}: Toward In-Network Computation with an In-Network Cache},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {June 2017},
 volume = {51},
 number = {2},
 month = apr,
 year = {2017},
 issn = {0163-5980},
 pages = {795--809},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/3093315.3037731},
 doi = {10.1145/3093315.3037731},
 acmid = {3037731},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {in-network caching, programmable network devices},
} 


@inproceedings{nopaxos,
 author = {Li, Jialin and Michael, Ellis and Sharma, Naveen Kr. and Szekeres, Adriana and Ports, Dan R. K.},
 title = {Just Say No to {Paxos} Overhead: Replacing Consensus with Network Ordering},
 booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
 series = {OSDI'16},
 year = {2016},
 isbn = {978-1-931971-33-1},
 location = {Savannah, GA, USA},
 pages = {467--483},
 numpages = {17},
 url = {http://dl.acm.org/citation.cfm?id=3026877.3026914},
 acmid = {3026914},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
} 

@MISC{CISCOMarket, 
title = {The Market Need for 40 Gigabit Ethernet}, howpublished = {\url{http://www.cisco.com/c/en/us/products/collateral/switches/catalyst-6500-series-switches/white_paper_c11-696667.html}}
}

@inproceedings{Gottlieb:1982:NUM:800048.801711,
 author = {Gottlieb, Allan and Grishman, Ralph and Kruskal, Clyde P. and McAuliffe, Kevin P. and Rudolph, Larry and Snir, Marc},
 title = {The {NYU} {Ultracomputer} Mdash;Designing a {MIMD}, Shared-memory Parallel Machine},
 booktitle = {Proceedings of the 9th Annual Symposium on Computer Architecture},
 series = {ISCA '82},
 year = {1982},
 location = {Austin, Texas, USA},
 pages = {27--42},
 numpages = {16},
 url = {http://dl.acm.org/citation.cfm?id=800048.801711},
 acmid = {801711},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 


@article{keskar2016large,
	title={On large-batch training for deep learning: Generalization gap and sharp minima},
	author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	journal={arXiv preprint arXiv:1609.04836},
	year={2016}
}

@article{lecun1524efficient,
	title={Efficient BackProp in Neural Networks: Tricks of the Trade},
	author={LeCun, Y and Bottou, L and Orr, G},
	journal={Lecture Notes in Computer Science},
	volume={1524}
}

@inproceedings{nesterov1983method,
	title={A method for unconstrained convex minimization problem with the rate of convergence O (1/k2)},
	author={Nesterov, Yurii},
	booktitle={Doklady an SSSR},
	volume={269},
	number={3},
	pages={543--547},
	year={1983}
}

@inproceedings {rdma,
author = {Anuj Kalia and Michael Kaminsky and David G. Andersen},
title = {Design Guidelines for High Performance {RDMA} Systems},
booktitle = {2016 {USENIX} Annual Technical Conference ({USENIX} {ATC} 16)},
year = {2016},
isbn = {978-1-931971-30-0},
address = {Denver, CO},
pages = {437--450},
url = {https://www.usenix.org/conference/atc16/technical-sessions/presentation/kalia},
publisher = {{USENIX} Association},
}


@misc{inception-resnet,
	Author = {Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi},
	Title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
	Year = {2016},
	Eprint = {arXiv:1602.07261},
}




@article{ResNext,
	author    = {Saining Xie and
	Ross B. Girshick and
	Piotr Doll{\'{a}}r and
	Zhuowen Tu and
	Kaiming He},
	title     = {Aggregated Residual Transformations for Deep Neural Networks},
	journal   = {CoRR},
	volume    = {abs/1611.05431},
	year      = {2016},
	url       = {http://arxiv.org/abs/1611.05431},
	timestamp = {Wed, 07 Jun 2017 14:41:38 +0200},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/XieGDTH16},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{inception-v3,
	author    = {Christian Szegedy and
	Vincent Vanhoucke and
	Sergey Ioffe and
	Jonathon Shlens and
	Zbigniew Wojna},
	title     = {Rethinking the Inception Architecture for Computer Vision},
	journal   = {CoRR},
	volume    = {abs/1512.00567},
	year      = {2015},
	url       = {http://arxiv.org/abs/1512.00567},
	timestamp = {Wed, 07 Jun 2017 14:40:22 +0200},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SzegedyVISW15},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Thakur:2005:OCC:2747766.2747771,
	author = {Thakur, Rajeev and Rabenseifner, Rolf and Gropp, William},
	title = {Optimization of Collective Communication Operations in MPICH},
	journal = {Int. J. High Perform. Comput. Appl.},
	issue_date = {February  2005},
	volume = {19},
	number = {1},
	month = feb,
	year = {2005},
	issn = {1094-3420},
	pages = {49--66},
	numpages = {18},
	url = {http://dx.doi.org/10.1177/1094342005051521},
	doi = {10.1177/1094342005051521},
	acmid = {2747771},
	publisher = {Sage Publications, Inc.},
	address = {Thousand Oaks, CA, USA},
	keywords = {Collective communication, MPI, message passing, reduction},
}

@inproceedings{tensorflow,
author = {Mart{\'\i}n Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
title = {TensorFlow: A System for Large-Scale Machine Learning},
booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {265--283},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
publisher = {{USENIX} Association},
}

@inproceedings{revisitSGD,
title = {Revisiting Distributed Synchronous SGD},
author  = {Jianmin Chen and Rajat Monga and Samy Bengio and Rafal Jozefowicz},
year  = {2016},
URL = {https://arxiv.org/abs/1604.00981},
booktitle = {International Conference on Learning Representations Workshop Track}
}

@inproceedings{projectAdam,
author = {Trishul Chilimbi and Yutaka Suzue and Johnson Apacible and Karthik Kalyanaraman},
title = {Project Adam: Building an Efficient and Scalable Deep Learning Training System},
booktitle = {11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
year = {2014},
isbn = { 978-1-931971-16-4},
address = {Broomfield, CO},
pages = {571--582},
url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/chilimbi},
publisher = {{USENIX} Association},
}

@inproceedings{googleDNN,
 author = {Dean, Jeffrey and Corrado, Greg S. and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V. and Mao, Mark Z. and Ranzato, Marc'Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y.},
 title = {Large Scale Distributed Deep Networks},
 booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
 series = {NIPS'12},
 year = {2012},
 location = {Lake Tahoe, Nevada},
 pages = {1223--1231},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2999134.2999271},
 acmid = {2999271},
 publisher = {Curran Associates Inc.},
 address = {USA},
}


@misc{nvidia-1080ti,
title= {NVIDIA 1080 Ti advertised price},
howpublished={https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080-ti}
}

@misc{nvidia-v100,
title= {NVIDIA V100 advertised price},
howpublished={https://www.thinkmate.com/product/nvidia/900-2g500-0000-000}
}

@misc{mellanox-eth,
title= {Mellanox Ethernet card prices},
howpublished={https://store.mellanox.com/categories/
adapters/ethernet-adapter-cards.html}
}

@misc{mellanox-cable,
title= {Mellanox Ethernet cable prices},
howpublished={https://store.mellanox.com/categories/
interconnect/ethernet-cables/direct-attach-copper-cables.html}
}

@misc{mellanox-splitter,
title= {Mellanox Ethernet splitter prices},
howpublished={https://store.mellanox.com/categories/interconnect/ethernet-cables/dac-splitter-cables.html}
}

@misc{worker-price,
title= {SuperMicro worker node price},
howpublished={https://www.thinkmate.com/system/
superserver-1028gq-tr}
}

@misc{phub-price,
title= {SuperMicro PHub node price},
howpublished={https://www.thinkmate.com/system/
superserver-6038r-txr}
}

@misc{arista-price,
title= {Arista 7060CX-32S price},
howpublished={https://goo.gl/cqyBtA}
}

@misc{MXNetont97:online,
author = {},
title = {MXNet on the Cloud — mxnet documentation},
howpublished = {https://mxnet.incubator.apache.org/faq/cloud.html?highlight=ec2},
month = {},
year = {},
note = {(Accessed on 12/07/2018)}
}

@misc{pytorchr46:online,
author = {},
title = {pytorch/resnet50\_trainer.py at master · pytorch/pytorch},
howpublished = {https://github.com/pytorch/pytorch/blob/master/caffe2/python/examples/resnet50_trainer.py},
month = {},
year = {},
note = {(Accessed on 12/07/2018)}
}


@misc{GoogleCl74:online,
author = {},
title = {Google Cloud Training. Google Cloud},
howpublished = {https://cloud.google.com/training/courses/machine-learning-tensorflow-gcp},
month = {},
year = {},
note = {(Accessed on 03/04/2019)}
}

@misc{MachineL50:online,
author = {},
title = {Machine Learning Studio. Microsoft Azure},
howpublished = {https://azure.microsoft.com/en-us/services/machine-learning-studio/},
month = {},
year = {},
note = {(Accessed on 12/06/2018)}
}

@misc{DeepLear23:online,
author = {},
title = {Deep Learning on AWS},
howpublished = {https://aws.amazon.com/deep-learning/},
month = {},
year = {},
note = {(Accessed on 12/06/2018)}
}


@inproceedings{collectivesOptimization,
author = {Rabenseifner, Rolf},
year = {2004},
month = {06},
pages = {1-9},
title = {Optimization of Collective Reduction Operations},
doi = {10.1007/978-3-540-24685-5_1}
}

@article{bala1995ccl,
  title={CCL: A portable and tunable collective communication library for scalable parallel computers},
  author={Bala, Vasanth and Bruck, Jehoshua and Cypher, Robert and Elustondo, Pablo and Ho, Alex and Ho, Ching-Tien and Kipnis, Shlomo and Snir, Marc},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={6},
  number={2},
  pages={154--164},
  year={1995},
  publisher={IEEE}
}

@article{blum2000architectures,
  title={Architectures and message-passing algorithms for cluster computing: Design and performance},
  author={Blum, Edward K and Wang, Xin and Leung, Patrick},
  journal={Parallel Computing},
  volume={26},
  number={2-3},
  pages={313--332},
  year={2000},
  publisher={Elsevier}
}


@phdthesis{Sack:2011:SCM:2522220,
 author = {Sack, Paul D.},
 advisor = {Gropp, William},
 title = {Scalable Collective Message-passing Algorithms},
 year = {2011},
 isbn = {978-1-267-27678-0},
 note = {AAI3503864},
 publisher = {University of Illinois at Urbana-Champaign},
 address = {Champaign, IL, USA},
} 



@inproceedings{Bilal2012ACS,
  title={A Comparative Study Of Data Center Network Architectures},
  author={Kashif Bilal and Samee Ullah Khan and Joanna Kolodziej and Limin Zhang and Khizar Hayat and Sajjad Ahmad Madani and Nasro Min-Allah and Lizhe Wang and Dan Chen},
  booktitle={ECMS},
  year={2012}
}


@inproceedings{cui2016geeps,
  title={GeePS: Scalable deep learning on distributed GPUs with a GPU-specialized parameter server},
  author={Cui, Henggang and Zhang, Hao and Ganger, Gregory R and Gibbons, Phillip B and Xing, Eric P},
  booktitle={Proceedings of the Eleventh European Conference on Computer Systems},
  pages={4},
  year={2016},
  organization={ACM}
}

@misc{Operatio73:online,
author = {},
title = {Operations - NCCL 2.3.4 documentation},
howpublished = {https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/usage/operations.html},
month = {},
year = {},
note = {(Accessed on 12/07/2018)}
}

@article{Sergeev2018HorovodFA,
  title={Horovod: fast and easy distributed deep learning in TensorFlow},
  author={Alexander Sergeev and Mike Del Balso},
  journal={CoRR},
  year={2018},
  volume={abs/1802.05799}
}


@misc{Placemen48:online,
author = {},
title = {Placement Groups - Amazon Elastic Compute Cloud},
howpublished = {https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html},
month = {},
year = {},
note = {(Accessed on 12/07/2018)}
}

@inproceedings{eyeQ,
author = {Jeyakumar, Vimalkumar and Alizadeh, Mohammad and Mazières, David and Prabhakar, Balaji and Kim, Changhoon},
year = {2012},
month = {06},
pages = {8-8},
title = {EyeQ: practical network performance isolation for the multi-tenant cloud}
}

@misc{27Octobe15:online,
author = {},
title = {27 | October | 2011 | Mixpanel Engineering},
howpublished = {https://engineering.mixpanel.com/2011/10/27/},
month = {},
year = {},
note = {(Accessed on 12/07/2018)}
}

@misc{Introduc22:online,
author = {},
title = {Introducing Dynamic Training for deep learning with Amazon EC2 | AWS Machine Learning Blog},
howpublished = {https://aws.amazon.com/blogs/machine-learning/introducing-dynamic-training-for-deep-learning-with-amazon-ec2/},
month = {},
year = {},
note = {(Accessed on 12/07/2018)}
}

@misc{AzureBat16:online,
author = {},
title = {Azure Batch AI | Train AI Models | Microsoft Azure},
howpublished = {https://azure.microsoft.com/en-us/services/batch-ai/},
month = {},
year = {},
note = {(Accessed on 12/07/2018)}
}

@misc{MXNetont32:online,
author = {},
title = {MXNet on the Cloud — mxnet documentation},
howpublished = {https://mxnet.incubator.apache.org/faq/cloud.html?highlight=ec2},
month = {},
year = {},
note = {(Accessed on 12/07/2018)}
}

@misc{pytorchr46:online,
author = {},
title = {pytorch/resnet50\_trainer.py at master | pytorch/pytorch},
howpublished = {https://github.com/pytorch/pytorch/blob/master/caffe2/python/examples/resnet50_trainer.py},
month = {},
year = {},
note = {(Accessed on 12/07/2018)}
}

@misc{benchmar10:online,
author = {},
title = {tensorflow/benchmarks},
howpublished = {https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks},
month = {},
year = {},
note = {(Accessed on 12/07/2018)}
}

@inproceedings{Shieh:2010:SPI:1863103.1863104,
 author = {Shieh, Alan and Kandula, Srikanth and Greenberg, Albert and Kim, Changhoon},
 title = {Seawall: Performance Isolation for Cloud Datacenter Networks},
 booktitle = {Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud Computing},
 series = {HotCloud'10},
 year = {2010},
 location = {Boston, MA},
 pages = {1--1},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=1863103.1863104},
 acmid = {1863104},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA}
 }
 
 @inproceedings{Shieh:2011:SDC:1972457.1972489,
 author = {Shieh, Alan and Kandula, Srikanth and Greenberg, Albert and Kim, Changhoon and Saha, Bikas},
 title = {Sharing the Data Center Network},
 booktitle = {Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation},
 series = {NSDI'11},
 year = {2011},
 location = {Boston, MA},
 pages = {309--322},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=1972457.1972489},
 acmid = {1972489},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA}
} 
 
 @inproceedings{Graham:2016:SHA:3018058.3018059,
 author = {Graham, Richard L. and Bureddy, Devendar and Lui, Pak and Rosenstock, Hal and Shainer, Gilad and Bloch, Gil and Goldenerg, Dror and Dubman, Mike and Kotchubievsky, Sasha and Koushnir, Vladimir and Levi, Lion and Margolin, Alex and Ronen, Tamir and Shpiner, Alexander and Wertheim, Oded and Zahavi, Eitan},
 title = {Scalable Hierarchical Aggregation Protocol (SHArP): A Hardware Architecture for Efficient Data Reduction},
 booktitle = {Proceedings of the First Workshop on Optimization of Communication in HPC},
 series = {COM-HPC '16},
 year = {2016},
 isbn = {978-1-5090-3829-9},
 location = {Salt Lake City, Utah},
 pages = {1--10},
 numpages = {10},
 url = {https://doi.org/10.1109/COM-HPC.2016.6},
 doi = {10.1109/COM-HPC.2016.6},
 acmid = {3018059},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
} 

@inproceedings{Geng:2018:HHP:3229543.3229544,
 author = {Geng, Jinkun and Li, Dan and Cheng, Yang and Wang, Shuai and Li, Junfeng},
 title = {HiPS: Hierarchical Parameter Synchronization in Large-Scale Distributed Machine Learning},
 booktitle = {Proceedings of the 2018 Workshop on Network Meets AI and ML},
 series = {NetAI'18},
 year = {2018},
 isbn = {978-1-4503-5911-5},
 location = {Budapest, Hungary},
 pages = {1--7},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/3229543.3229544},
 doi = {10.1145/3229543.3229544},
 acmid = {3229544},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{softiWarp,
author = {Metzler, Bernard and Frey, Philip and Trivedi, Animesh},
year = {2010},
month = {03},
pages = {},
title = {SoftiWARP – Project Update: A Software iWARP Driver for OpenFabrics},
doi = {10.13140/2.1.4422.3363}
}

@inproceedings{luo2017motivating,
  title={Motivating in-network aggregation for distributed deep neural network training},
  author={Luo, Liang and Liu, Ming and Nelson, Jacob and Ceze, Luis and Phanishayee, Amar and Krishnamurthy, Arvind},
  booktitle={Workshop on Approximate Computing Across the Stack},
  year={2017}
}

@misc{softroce,
author = {},
title = {Soft RDMA over Ethernet (RoCE) Driver},
howpublished = {https://github.com/SoftRoCE/rxe-dev/wiki/rxe-dev:-Home},
month = {},
year = {},
note = {(Accessed on 12/16/2018)}
}

@inproceedings {Litz,
author = {Aurick Qiao and Abutalib Aghayev and Weiren Yu and Haoyang Chen and Qirong Ho and Garth A. Gibson and Eric P. Xing},
title = {Litz: Elastic Framework for High-Performance Distributed Machine Learning},
booktitle = {2018 {USENIX} Annual Technical Conference ({USENIX} {ATC} 18)},
year = {2018},
isbn = {978-1-931971-44-7},
address = {Boston, MA},
pages = {631--644},
url = {https://www.usenix.org/conference/atc18/presentation/qiao},
publisher = {{USENIX} Association},
}

@inproceedings{orpheus,
author = {Xie, Pengtao and Kyu Kim, Jin and Ho, Qirong and Yu, Yaoliang and Xing, Eric},
year = {2018},
month = {10},
pages = {1-13},
title = {Orpheus: Efficient Distributed Machine Learning via System and Algorithm Co-design},
doi = {10.1145/3267809.3267810}
}


@misc{TopEight59:online,
author = {},
title = {Top Eight Data Center Trends For Keeping up with High Data Bandwidth Demand - Marvell - Blog Marvell – Blog},
howpublished = {https://blogs.marvell.com/2017/04/top-eight-data-center-trends-for-keeping-up-with-high-data-bandwidth-demand/},
month = {},
year = {},
note = {(Accessed on 02/25/2019)}
}

@article{vinh2010information,
  title={Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance},
  author={Vinh, Nguyen Xuan and Epps, Julien and Bailey, James},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Oct},
  pages={2837--2854},
  year={2010}
}

@misc{affinity2Distance,
author = {},
title = {4.8. Pairwise metrics, Affinities and Kernels. scikit-learn 0.20.2 documentation},
howpublished = {https://scikit-learn.org/stable/modules/metrics.html},
month = {},
year = {},
note = {(Accessed on 02/06/2019)}
}

@misc{NTTTCP,
author = {},
title = {Microsoft/ntttcp-for-linux: A Linux network throughput multiple-thread benchmark tool.},
howpublished = {https://github.com/Microsoft/ntttcp-for-linux},
month = {},
year = {},
note = {(Accessed on 02/06/2019)}
}

@ARTICLE{packetTrain, 
author={Ningning Hu and P. Steenkiste}, 
journal={IEEE Journal on Selected Areas in Communications}, 
title={Evaluation and characterization of available bandwidth probing techniques}, 
year={2003}, 
volume={21}, 
number={6}, 
pages={879-894}, 
keywords={telecommunication traffic;Internet;packet switching;performance evaluation;channel capacity;telecommunication links;queueing theory;bandwidth probing techniques;packet pair mechanism;bottleneck link capacity measurement;simulations;competing network traffic;probing packet gap;single-hop network;gap model;available bandwidth measurement;initial gap increasing method;packet transmission rate method;Internet measurements;Pathload;measurement accuracy;probing packet size;probing packet train length;queue size;Bandwidth;Telecommunication traffic;Traffic control;IP networks;Internet;Measurement techniques;Length measurement;Size measurement;Computerized monitoring;Computer science}, 
doi={10.1109/JSAC.2003.814505}, 
ISSN={0733-8716}, 
month={Aug},}

@misc{Createan37:online,
author = {},
title = {Create an Azure virtual machine with Accelerated Networking | Microsoft Docs},
howpublished = {https://docs.microsoft.com/en-us/azure/virtual-network/create-vm-accelerated-networking-cli},
month = {},
year = {},
note = {(Accessed on 01/06/2019)}
}

@misc{Enablean80:online,
author = {},
title = {Enable and Configure Enhanced Networking for EC2 Instances},
howpublished = {https://aws.amazon.com/premiumsupport/knowledge-center/enable-configure-enhanced-networking/},
month = {},
year = {},
note = {(Accessed on 01/06/2019)}
}

@misc{NVIDIAnc61:online,
author = {},
title = {NVIDIA/nccl-tests: NCCL Tests},
howpublished = {https://github.com/NVIDIA/nccl-tests},
month = {},
year = {},
note = {(Accessed on 01/06/2019)}
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: a system for large-scale machine learning.},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={OSDI},
  volume={16},
  pages={265--283},
  year={2016}
}

@inproceedings{Kraska:2018:CLI:3183713.3196909,
 author = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
 title = {The Case for Learned Index Structures},
 booktitle = {Proceedings of the 2018 International Conference on Management of Data},
 series = {SIGMOD '18},
 year = {2018},
 isbn = {978-1-4503-4703-7},
 location = {Houston, TX, USA},
 pages = {489--504},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3183713.3196909},
 doi = {10.1145/3183713.3196909},
 acmid = {3196909},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {b-tree, bloom-filter, cdf, hash-map, index structures, learned data structures, learned index, learned index structure, linear regression, mixture of experts, neural net},
} 

@article{mirhoseini2017device,
  title={Device placement optimization with reinforcement learning},
  author={Mirhoseini, Azalia and Pham, Hieu and Le, Quoc V and Steiner, Benoit and Larsen, Rasmus and Zhou, Yuefeng and Kumar, Naveen and Norouzi, Mohammad and Bengio, Samy and Dean, Jeff},
  journal={arXiv preprint arXiv:1706.04972},
  year={2017}
}

@INPROCEEDINGS{8366945, 
author={B. Bodin and L. Nardi and H. Wagstaff and P. H. J. Kelly and M. O'Boyle}, 
booktitle={2018 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
title={Algorithmic Performance-Accuracy Trade-off in 3D Vision Applications}, 
year={2018}, 
volume={}, 
number={}, 
pages={123-124}, 
keywords={augmented reality;public domain software;robot vision;SLAM (robots);augmented reality systems;SLAM algorithms;SLAMBench;publicly-available software framework;large-scale performance evaluation;3D vision applications;simultaneous localisation and mapping;AR;Benchmark testing;Simultaneous localization and mapping;Three-dimensional displays;Software algorithms;Mobile handsets;Hardware;Computer vision;SLAM;performance analysis;domain specific exploration;embedded systems}, 
doi={10.1109/ISPASS.2018.00024}, 
ISSN={}, 
month={April},}

@article{chen2018learning,
  title={Learning to Optimize Tensor Programs},
  author={Chen, Tianqi and Zheng, Lianmin and Yan, Eddie and Jiang, Ziheng and Moreau, Thierry and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  journal={arXiv preprint arXiv:1805.08166},
  year={2018}
}

@inproceedings {222575,
author = {Tianqi Chen and Thierry Moreau and Ziheng Jiang and Lianmin Zheng and Eddie Yan and Haichen Shen and Meghan Cowan and Leyuan Wang and Yuwei Hu and Luis Ceze and Carlos Guestrin and Arvind Krishnamurthy},
title = {{TVM}: An Automated End-to-End Optimizing Compiler for Deep Learning},
booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
year = {2018},
isbn = {978-1-931971-47-8},
address = {Carlsbad, CA},
pages = {578--594},
url = {https://www.usenix.org/conference/osdi18/presentation/chen},
publisher = {{USENIX} Association},
}

@misc{Nowanyon13:online,
author = {},
title = {Now anyone can train Imagenet in 18 minutes · fast.ai},
howpublished = {https://www.fast.ai/2018/08/10/fastai-diu-imagenet/},
month = {},
year = {},
note = {(Accessed on 12/20/2018)}
}

@misc{sridharan2018scaleout,
    title={On Scale-out Deep Learning Training for Cloud and HPC},
    author={Srinivas Sridharan and Karthikeyan Vaidyanathan and Dhiraj Kalamkar and Dipankar Das and Mikhail E. Smorkalov and Mikhail Shiryaev and Dheevatsa Mudigere and Naveen Mellempudi and Sasikanth Avancha and Bharat Kaul and Pradeep Dubey},
    year={2018},
    eprint={1801.08030},
    archivePrefix={arXiv},
    primaryClass={cs.DC}
}

@misc{jia2018highly,
    title={Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes},
    author={Xianyan Jia and Shutao Song and Wei He and Yangzihao Wang and Haidong Rong and Feihu Zhou and Liqiang Xie and Zhenyu Guo and Yuanzhou Yang and Liwei Yu and Tiegang Chen and Guangxiao Hu and Shaohuai Shi and Xiaowen Chu},
    year={2018},
    eprint={1807.11205},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{Shen2018NexusA,
  title={Nexus : A GPU Cluster for Accelerating Neural Networks for Video Analysis},
  author={Haichen Shen and Yuchen Jin and Bingyu Kong and M. Philipose and A. KrishnaMurthy and Ravi Sundaram},
  year={2018}
}

@article{lin2017deep,
  title={Deep gradient compression: Reducing the communication bandwidth for distributed training},
  author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  journal={arXiv preprint arXiv:1712.01887},
  year={2017}
}


@article{xie2014large,
  title={Large scale distributed multiclass logistic regression},
  author={Xie, Pengtao and Kim, Jin Kyu and Xing, Eric P},
  journal={CoRR, abs/1409.5705},
  year={2014},
  publisher={Citeseer}
}

@misc{NetlinkW13:online,
author = {},
title = {Netlink Wikipedia},
howpublished = {https://en.wikipedia.org/wiki/Netlink##cite_note-4},
month = {},
year = {},
note = {(Accessed on 12/22/2018)}
}

@article{mathis2003web100,
  title={Web100: extended TCP instrumentation for research, education and diagnosis},
  author={Mathis, Matt and Heffner, John and Reddy, Raghu},
  journal={ACM SIGCOMM Computer Communication Review},
  volume={33},
  number={3},
  pages={69--79},
  year={2003},
  publisher={ACM}
}

@inproceedings{Pesterev:2012:INC:2168836.2168870,
 author = {Pesterev, Aleksey and Strauss, Jacob and Zeldovich, Nickolai and Morris, Robert T.},
 title = {Improving Network Connection Locality on Multicore Systems},
 booktitle = {Proceedings of the 7th ACM European Conference on Computer Systems},
 series = {EuroSys '12},
 year = {2012},
 isbn = {978-1-4503-1223-3},
 location = {Bern, Switzerland},
 pages = {337--350},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2168836.2168870},
 doi = {10.1145/2168836.2168870},
 acmid = {2168870},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache misses, multi-core, packet processing},
} 

@inproceedings{kong2018improving,
  title={Improving TCP Congestion Control with Machine Intelligence},
  author={Kong, Yiming and Zang, Hui and Ma, Xiaoli},
  booktitle={Proceedings of the 2018 Workshop on Network Meets AI \& ML},
  pages={60--66},
  year={2018},
  organization={ACM}
}

@article{haeri2006adaptive,
  title={Adaptive model predictive TCP delay-based congestion control},
  author={Haeri, Mohammad and Rad, Amir Hamed Mohsenian},
  journal={Computer Communications},
  volume={29},
  number={11},
  pages={1963--1978},
  year={2006},
  publisher={Elsevier}
}

@article{biaz1998performance,
  title={Performance of tcp congestion predictors as loss predictors},
  author={Biaz, Saad and Vaidya, Nitin H},
  journal={Department of Computer Science Technical Report},
  volume={98},
  number={7},
  year={1998}
}

@inproceedings{li2016learning,
  title={Learning-based and data-driven tcp design for memory-constrained iot},
  author={Li, Wei and Zhou, Fan and Meleis, Waleed and Chowdhury, Kaushik},
  booktitle={Distributed Computing in Sensor Systems (DCOSS), 2016 International Conference on},
  pages={199--205},
  year={2016},
  organization={IEEE}
}

@misc{dmlcpsli50:online,
author = {},
title = {dmlc/ps-lite: A lightweight parameter server interface},
howpublished = {https://github.com/dmlc/ps-lite},
month = {},
year = {},
note = {(Accessed on 12/22/2018)}
}

@misc{facebook35:online,
author = {},
title = {facebookincubator/gloo: Collective communications library with various primitives for multi-machine training.},
howpublished = {https://github.com/facebookincubator/gloo},
month = {},
year = {},
note = {(Accessed on 12/22/2018)}
}

@misc{Evaluati26:online,
author = {},
title = {Evaluation Of Major Deep Learning Frameworks With Benchmark},
howpublished = {https://www.analyticsindiamag.com/evaluation-of-major-deep-learning-frameworks/},
month = {},
year = {},
note = {(Accessed on 12/22/2018)}
}

@article{bradley2000constrained,
  title={Constrained k-means clustering},
  author={Bradley, PS and Bennett, KP and Demiriz, Ayhan},
  journal={Microsoft Research, Redmond},
  volume={20},
  number={0},
  pages={0},
  year={2000}
}

@inproceedings{arthur2007k,
  title={k-means++: The advantages of careful seeding},
  author={Arthur, David and Vassilvitskii, Sergei},
  booktitle={Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
  pages={1027--1035},
  year={2007},
  organization={Society for Industrial and Applied Mathematics}
}

@article{Dabek:2004:VDN:1030194.1015471,
 author = {Dabek, Frank and Cox, Russ and Kaashoek, Frans and Morris, Robert},
 title = {Vivaldi: A Decentralized Network Coordinate System},
 journal = {SIGCOMM Comput. Commun. Rev.},
 issue_date = {October 2004},
 volume = {34},
 number = {4},
 month = aug,
 year = {2004},
 issn = {0146-4833},
 pages = {15--26},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1030194.1015471},
 doi = {10.1145/1030194.1015471},
 acmid = {1015471},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Vivaldi, internet topology, network coordinates},
} 

@unknown{facebookDLHardware,
author = {Park, Jongsoo and Naumov, Maxim and Basu, Protonu and Deng, Summer and Kalaiah, Aravind and Khudia, Daya and Law, James and Malani, Parth and Malevich, Andrey and Nadathur, Satish and Miguel Pino, Juan and Schatz, Martin and Sidorov, Alexander and Sivakumar, Viswanath and Tulloch, Andrew and Wang, Xiaodong and Wu, Yiming and Yuen, Hector and Diril, Utku and Smelyanskiy, Mikhail},
year = {2018},
month = {11},
pages = {},
title = {Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications}
}

@article{DBLP:journals/corr/abs-1711-00489,
  author    = {Samuel L. Smith and
               Pieter{-}Jan Kindermans and
               Quoc V. Le},
  title     = {Don't Decay the Learning Rate, Increase the Batch Size},
  journal   = {CoRR},
  volume    = {abs/1711.00489},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.00489},
  archivePrefix = {arXiv},
  eprint    = {1711.00489},
  timestamp = {Mon, 13 Aug 2018 16:46:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-00489},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{You:2018:ITM:3225058.3225069,
 author = {You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
 title = {ImageNet Training in Minutes},
 booktitle = {Proceedings of the 47th International Conference on Parallel Processing},
 series = {ICPP 2018},
 year = {2018},
 isbn = {978-1-4503-6510-9},
 location = {Eugene, OR, USA},
 pages = {1:1--1:10},
 articleno = {1},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3225058.3225069},
 doi = {10.1145/3225058.3225069},
 acmid = {3225069},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Distributed Machine Learning, Fast Deep Neural Networks Training},
} 

@article{xie2015distributed,
  title={Distributed machine learning via sufficient factor broadcasting},
  author={Xie, Pengtao and Kim, Jin Kyu and Zhou, Yi and Ho, Qirong and Kumar, Abhimanu and Yu, Yaoliang and Xing, Eric},
  journal={arXiv preprint arXiv:1511.08486},
  year={2015}
}

@article{DBLP:journals/corr/abs-1711-04325,
  author    = {Takuya Akiba and
               Shuji Suzuki and
               Keisuke Fukuda},
  title     = {Extremely Large Minibatch {SGD:} Training ResNet-50 on ImageNet in
               15 Minutes},
  journal   = {CoRR},
  volume    = {abs/1711.04325},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.04325},
  archivePrefix = {arXiv},
  eprint    = {1711.04325},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-04325},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1811-05233,
  author    = {Hiroaki Mikami and
               Hisahiro Suganuma and
               Pongsakorn U.{-}Chupala and
               Yoshiki Tanaka and
               Yuichi Kageyama},
  title     = {ImageNet/ResNet-50 Training in 224 Seconds},
  journal   = {CoRR},
  volume    = {abs/1811.05233},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.05233},
  archivePrefix = {arXiv},
  eprint    = {1811.05233},
  timestamp = {Sat, 24 Nov 2018 17:52:00 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-05233},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sun2019optimizing,
  title={Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes},
  author={Sun, Peng and Feng, Wansen and Han, Ruobing and Yan, Shengen and Wen, Yonggang},
  journal={arXiv preprint arXiv:1902.06855},
  year={2019}
}

@misc{sagemaker,
author = {},
title = {Amazon EC2 F1 Instances},
howpublished = {https://aws.amazon.com/ec2/instance-types/f1},
month = {},
year = {},
note = {(Accessed on 03/04/2019)}
}

@inproceedings{brainwave,
  title={A configurable cloud-scale DNN processor for real-time AI},
  author={Fowers, Jeremy and Ovtcharov, Kalin and Papamichael, Michael and Massengill, Todd and Liu, Ming and Lo, Daniel and Alkalay, Shlomi and Haselman, Michael and Adams, Logan and Ghandi, Mahdi and others},
  booktitle={Proceedings of the 45th Annual International Symposium on Computer Architecture},
  pages={1--14},
  year={2018},
  organization={IEEE Press}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings {perfVariance,
author = {Aleksander Maricq and Dmitry Duplyakin and Ivo Jimenez and Carlos Maltzahn and Ryan Stutsman and Robert Ricci},
title = {Taming Performance Variability},
booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
year = {2018},
isbn = {978-1-931971-47-8},
address = {Carlsbad, CA},
pages = {409--425},
url = {https://www.usenix.org/conference/osdi18/presentation/maricq},
publisher = {{USENIX} Association},
}

@inproceedings{cloudVariance1,
 author = {Farley, Benjamin and Juels, Ari and Varadarajan, Venkatanathan and Ristenpart, Thomas and Bowers, Kevin D. and Swift, Michael M.},
 title = {More for Your Money: Exploiting Performance Heterogeneity in Public Clouds},
 booktitle = {Proceedings of the Third ACM Symposium on Cloud Computing},
 series = {SoCC '12},
 year = {2012},
 isbn = {978-1-4503-1761-0},
 location = {San Jose, California},
 pages = {20:1--20:14},
 articleno = {20},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2391229.2391249},
 doi = {10.1145/2391229.2391249},
 acmid = {2391249},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cloud computing, heterogeneity, virtual machine migration},
} 


@Inproceedings{data-center-tcp-dctcp,
author = {Alizadeh, Mohammad and Greenberg, Albert and Maltz, Dave and Padhye, Jitu and Pate, Parveen and Prabhakar, Balaji and  and Sridharan, Murari},
title = {Data Center TCP (DCTCP)},
year = {2010},
month = {September},
abstract = {Cloud data centers host diverse applications, mixing workloads that require small predictable latency with others requiring large sustained throughput. In this environment, today’s state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal impairments that lead to high application latencies, rooted in TCP’s demands on the limited buffer space available in data center switches. For example, bandwidth hungry “background” flows build up queues at the switches, and thus impact the performance of latency sensitive “foreground” traffic. To address these problems, we propose DCTCP, a TCP-like protocol for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) in the network to provide multi-bit feedback to the end hosts. We evaluate DCTCP at 1 and 10Gbps speeds using commodity, shallow buffered switches. We find DCTCP delivers the same or better throughput than TCP, while using 90% less buffer space. Unlike TCP, DCTCP also provides high burst tolerance and low latency for short flows. In handling workloads derived from operational measurements, we found DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further, a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems.},
publisher = {ACM},
url = {https://www.microsoft.com/en-us/research/publication/data-center-tcp-dctcp/},
edition = {SIGCOMM 2010},
}

@inproceedings{Iosup:2011:PVP:2007336.2007402,
 author = {Iosup, Alexandru and Yigitbasi, Nezih and Epema, Dick},
 title = {On the Performance Variability of Production Cloud Services},
 booktitle = {Proceedings of the 2011 11th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
 series = {CCGRID '11},
 year = {2011},
 isbn = {978-0-7695-4395-6},
 pages = {104--113},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/CCGrid.2011.22},
 doi = {10.1109/CCGrid.2011.22},
 acmid = {2007402},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {cloud, performance, traces, analysis, performance variability, production cloud services},
} 

@inproceedings {222611,
author = {Wencong Xiao and Romil Bhardwaj and Ramachandran Ramjee and Muthian Sivathanu and Nipun Kwatra and Zhenhua Han and Pratyush Patel and Xuan Peng and Hanyu Zhao and Quanlu Zhang and Fan Yang and Lidong Zhou},
title = {Gandiva: Introspective Cluster Scheduling for Deep Learning},
booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
year = {2018},
isbn = {978-1-931971-47-8},
address = {Carlsbad, CA},
pages = {595--610},
url = {https://www.usenix.org/conference/osdi18/presentation/xiao},
publisher = {{USENIX} Association},
}

@inproceedings {201564,
author = {Kevin Hsieh and Aaron Harlap and Nandita Vijaykumar and Dimitris Konomis and Gregory R. Ganger and Phillip B. Gibbons and Onur Mutlu},
title = {Gaia: Geo-Distributed Machine Learning Approaching {LAN} Speeds},
booktitle = {14th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 17)},
year = {2017},
isbn = {978-1-931971-37-9},
address = {Boston, MA},
pages = {629--647},
url = {https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/hsieh},
publisher = {{USENIX} Association},
}

@misc{cano2016geodistributed,
    title={Towards Geo-Distributed Machine Learning},
    author={Ignacio Cano and Markus Weimer and Dhruv Mahajan and Carlo Curino and Giovanni Matteo Fumarola},
    year={2016},
    eprint={1603.09035},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{choblueconnect,
  title={BlueConnect: Novel Hierarchical All-Reduce on Multi-tired Network for Deep Learning},
  author={Cho, Minsik and Finkler, Ulrich and Kung, David},
  booktitle={LearningSys 2018},
  year = {2018}
}

@article{sysmlblueconnect,
  title={BLUECONNECT: Decomposing All-Reduce For Deep Learning On
Heterogeneous Network Hierarchy},
  author={Minsik Cho, Ulrich Finkler , David Kung},
  booktitle={SysML 2019},
  year = {2019}
}

@article{prioritybased,
  title={Priority-based Parameter Propagation for Distributed DNN Training},
  author={Anand Jayarajan, Jinliang Wei, Garth Gibson, Alexandra Fedorova, Gennady Pekhimenko},
  booktitle={SysML 2019},
  year = {2019}
}


@misc{Introduc92:online,
author = {},
title = {Introducing Amazon EC2 C5n Instances Featuring 100 Gbps of Network Bandwidth},
howpublished = {https://aws.amazon.com/about-aws/whats-new/2018/11/introducing-amazon-ec2-c5n-instances/},
month = {},
year = {},
note = {(Accessed on 03/07/2019)}
}

@misc{Introduc49:online,
author = {},
title = {Introducing the new HB and HC Azure VM sizes for HPC. Blog. Microsoft Azure},
howpublished = {https://azure.microsoft.com/en-us/blog/introducing-the-new-hb-and-hc-azure-vm-sizes-for-hpc/},
month = {},
year = {},
note = {(Accessed on 03/07/2019)}
}

@inproceedings{Jouppi:2017:IPA:3079856.3080246,
 author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
 title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
 booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
 series = {ISCA '17},
 year = {2017},
 isbn = {978-1-4503-4892-8},
 location = {Toronto, ON, Canada},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/3079856.3080246},
 doi = {10.1145/3079856.3080246},
 acmid = {3080246},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CNN, DNN, GPU, LSTM, MLP, RNN, TPU, TensorFlow, accelerator, deep learning, domain-specific architecture, neural network},
} 

@inproceedings {211249,
author = {Daniel Firestone and Andrew Putnam and Sambhrama Mundkur and Derek Chiou and Alireza Dabagh and Mike Andrewartha and Hari Angepat and Vivek Bhanu and Adrian Caulfield and Eric Chung and Harish Kumar Chandrappa and Somesh Chaturmohta and Matt Humphrey and Jack Lavier and Norman Lam and Fengfen Liu and Kalin Ovtcharov and Jitu Padhye and Gautham Popuri and Shachar Raindel and Tejas Sapre and Mark Shaw and Gabriel Silva and Madhan Sivakumar and Nisheeth Srivastava and Anshuman Verma and Qasim Zuhair and Deepak Bansal and Doug Burger and Kushagra Vaid and David A. Maltz and Albert Greenberg},
title = {Azure Accelerated Networking: SmartNICs in the Public Cloud},
booktitle = {15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18)},
year = {2018},
isbn = {978-1-931971-43-0},
address = {Renton, WA},
pages = {51--66},
url = {https://www.usenix.org/conference/nsdi18/presentation/firestone},
publisher = {{USENIX} Association},
}

@Inproceedings{VL2,
author = {Greenberg, Albert and Hamilton, James R. and Jain, Navendu and Kandula, Srikanth and  and Lahiri, Parantap and Maltz, Dave and  and },
title = {VL2: A Scalable and Flexible Data Center Network},
year = {2009},
month = {August},
abstract = {

To be agile and cost effective, data centers should allow dynamic resource allocation across large server pools. In particular, the data center network should enable any server to be assigned to any service. To meet these goals, we present VL2, a practical network architecture that scales to support huge data centers with uniform high capacity between servers, performance isolation between services, and Ethernet layer-2 semantics. VL2 uses (1) flat addressing to allow service instances to be placed anywhere in the network, (2) Valiant Load Balancing to spread traffic uniformly across network paths, and (3) end-system based address resolution to scale to large server pools, without introducing complexity to the network control plane. VL2’s design is driven by detailed measurements of traffic and fault data from a large operational cloud service provider. VL2’s implementation leverages proven network technologies, already available at low cost in high-speed hardware implementations, to build a scalable and reliable network architecture. As a result, VL2 networks can be deployed today, and we have built a working prototype. We evaluate the merits of the VL2 design using measurement, analysis, and experiments. Our VL2 prototype shuffles 2.7 TB of data among 75 servers in 395 seconds – sustaining a rate that is 94% of the maximum possible.

Invited paper in Research Highlights section of the newly re-formatted Communications of the ACM (CACM).


},
publisher = {Association for Computing Machinery, Inc.},
url = {https://www.microsoft.com/en-us/research/publication/vl2-a-scalable-and-flexible-data-center-network/},
edition = {SIGCOMM},
note = {Recognized as one of "the most important research results published in CS in recent years".},
}

@misc{nccl,
author = {},
title = {NVIDIA Collective Communications Library},
howpublished = {https://developer.nvidia.com/nccl}},
month = {},
year = {},
note = {(Accessed on 03/19/2019)}
}

@inproceedings{xie2018orpheus,
  title={Orpheus: Efficient distributed machine learning via system and algorithm co-design},
  author={Xie, Pengtao and Kim, Jin Kyu and Ho, Qirong and Yu, Yaoliang and Xing, Eric},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={1--13},
  year={2018},
  organization={ACM}
}

@misc{wang2018adaptive,
    title={Adaptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD},
    author={Jianyu Wang and Gauri Joshi},
    year={2018},
    eprint={1810.08313},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{burtscher2009fpc,
  title={FPC: A high-speed compressor for double-precision floating-point data},
  author={Burtscher, Martin and Ratanaworabhan, Paruj},
  journal={IEEE Transactions on Computers},
  volume={58},
  number={1},
  pages={18--31},
  year={2009},
  publisher={IEEE}
}

@article{lim20183lc,
  title={3LC: Lightweight and Effective Traffic Compression for Distributed Machine Learning},
  author={Lim, Hyeontaek and Andersen, David G and Kaminsky, Michael},
  journal={arXiv preprint arXiv:1802.07389},
  year={2018}
}


@article{hashemi2018tictac,
  title={TicTac: Accelerating Distributed Deep Learning with Communication Scheduling},
  author={Hashemi, Sayed Hadi and Jyothi, Sangeetha Abdu and Campbell, Roy H},
  journal={arXiv preprint arXiv:1803.03288},
  year={2018}
}

@article{harlap2018pipedream,
  title={Pipedream: Fast and efficient pipeline parallel dnn training},
  author={Harlap, Aaron and Narayanan, Deepak and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil and Ganger, Greg and Gibbons, Phil},
  journal={arXiv preprint arXiv:1806.03377},
  year={2018}
}

@article{jia2018beyond,
  title={Beyond data and model parallelism for deep neural networks},
  author={Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  journal={arXiv preprint arXiv:1807.05358},
  year={2018}
}

@article{dong2012high,
  title={High performance network virtualization with SR-IOV},
  author={Dong, Yaozu and Yang, Xiaowei and Li, Jianhui and Liao, Guangdeng and Tian, Kun and Guan, Haibing},
  journal={Journal of Parallel and Distributed Computing},
  volume={72},
  number={11},
  pages={1471--1480},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  booktitle={Advances in neural information processing systems},
  pages={3146--3154},
  year={2017}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

@book{wang2005support,
  title={Support vector machines: theory and applications},
  author={Wang, Lipo},
  volume={177},
  year={2005},
  publisher={Springer Science \& Business Media}
}

@book{seber2012linear,
  title={Linear regression analysis},
  author={Seber, George AF and Lee, Alan J},
  volume={329},
  year={2012},
  publisher={John Wiley \& Sons}
}

@misc{NewC5nIn6:online,
author = {},
title = {New C5n Instances with 100 Gbps Networking | AWS News Blog},
howpublished = {https://aws.amazon.com/blogs/aws/new-c5n-instances-with-100-gbps-networking/},
month = {},
year = {},
note = {(Accessed on 02/04/2020)}
}

@misc{Introduc9:online,
author = {},
title = {Introducing the new HB and HC Azure VM sizes for HPC | Azure Blog and Updates | Microsoft Azure},
howpublished = {https://azure.microsoft.com/en-us/blog/introducing-the-new-hb-and-hc-azure-vm-sizes-for-hpc/},
month = {},
year = {},
note = {(Accessed on 02/04/2020)}
}

@article{subgradient,
author = {Boyd, Stephen and Mutapcic, Almir},
year = {2003},
month = {01},
pages = {},
title = {Subgradient Methods},
volume = {2004},
journal = {lecture notes of EE392o, Stanford University, Autumn Quarter}
}